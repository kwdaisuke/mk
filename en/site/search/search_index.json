{"config":{"indexing":"full","lang":["fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Babushka ML Babushka ML is organized by Daisuke Kuwabara aiming at spreading the best practice for ML Who this course for? College Student just starting Data Science/Machine Learning journey Engineers who are interested in Data Engineer/Machine Learning Engineer Everyone interested in Data Science! How much it costs? Depends on your situations, the tuition is completely flexible. The estimation is $15/h but is is likely subject to your conditions/requirements Why choosing Babushka? Machine Learning is at the infant stage, and yet online schools are abundant with similar contents. As MSc in Data Science holder, I have experienced these online courses, which are useful to some extent, yet lacking the hands-on practices. Babushka ML is focusing only what brings economic values in your work/business","title":"Home"},{"location":"#welcome-to-babushka-ml","text":"Babushka ML is organized by Daisuke Kuwabara aiming at spreading the best practice for ML","title":"Welcome to Babushka ML "},{"location":"#who-this-course-for","text":"","title":"Who this course for?"},{"location":"#college-student-just-starting-data-sciencemachine-learning-journey","text":"","title":"College Student just starting Data Science/Machine Learning journey"},{"location":"#engineers-who-are-interested-in-data-engineermachine-learning-engineer","text":"","title":"Engineers who are interested in Data Engineer/Machine Learning Engineer"},{"location":"#everyone-interested-in-data-science","text":"","title":"Everyone interested in Data Science!"},{"location":"#how-much-it-costs","text":"Depends on your situations, the tuition is completely flexible. The estimation is $15/h but is is likely subject to your conditions/requirements","title":"How much it costs?"},{"location":"#why-choosing-babushka","text":"Machine Learning is at the infant stage, and yet online schools are abundant with similar contents. As MSc in Data Science holder, I have experienced these online courses, which are useful to some extent, yet lacking the hands-on practices. Babushka ML is focusing only what brings economic values in your work/business","title":"Why choosing Babushka?"},{"location":"course/","text":"Python Programming Introduction to Python3 Data Validation and settings management using Pydantic Object Oriented Programming Decorators and Callback Software Engineering Linux RESTful API with Fast API and Uvicorn Graph Database - NoSQL Docker and Operation System Kubernetes and its components Testing(Parametrize, Fixtures, Markers, Coverage) Production Networking TCP/IP model Machine Learning Engineer Dashboard using Streamlit Data Versioning with DVC CI/CD Workflows with GitHub Actions Google's MLOps Feature Store with Feast Pipelines with Kubeflow Monitoring ML Systems Data Engineer BigQuery and SQL with dbt Batch/Streaming Processing with Apache Beam Messaging System with Pub/Sub Apache Airflow with Cloud Composer Hadoop/Spark Ecosystem with Dataproc Data Science Probability and Statistics Mathematics Continuous Optimization Time-Series Analysis Survival Analysis using R Neural Network Parallel Computing in Python with Dask Convolutional Neural Network Deep Learning Price Babushka ML is focusing on human factors in the first place. As such, the price of each course is detemined by the dynamic pricing, which means you can directly negotiate up to your satisfaction. About Daisuke Kuwabara - Data Specialist at EPAM Systems - Master in Data Science - Google Cloud Professional Data Engineer - AWS Solution Architect Associates","title":"Course"},{"location":"course/#python-programming","text":"Introduction to Python3 Data Validation and settings management using Pydantic Object Oriented Programming Decorators and Callback","title":"Python Programming"},{"location":"course/#software-engineering","text":"Linux RESTful API with Fast API and Uvicorn Graph Database - NoSQL Docker and Operation System Kubernetes and its components Testing(Parametrize, Fixtures, Markers, Coverage) Production","title":"Software Engineering"},{"location":"course/#networking","text":"TCP/IP model","title":"Networking"},{"location":"course/#machine-learning-engineer","text":"Dashboard using Streamlit Data Versioning with DVC CI/CD Workflows with GitHub Actions Google's MLOps Feature Store with Feast Pipelines with Kubeflow Monitoring ML Systems","title":"Machine Learning Engineer"},{"location":"course/#data-engineer","text":"BigQuery and SQL with dbt Batch/Streaming Processing with Apache Beam Messaging System with Pub/Sub Apache Airflow with Cloud Composer Hadoop/Spark Ecosystem with Dataproc","title":"Data Engineer"},{"location":"course/#data-science","text":"Probability and Statistics Mathematics Continuous Optimization Time-Series Analysis Survival Analysis using R Neural Network Parallel Computing in Python with Dask Convolutional Neural Network Deep Learning","title":"Data Science"},{"location":"course/#price","text":"Babushka ML is focusing on human factors in the first place. As such, the price of each course is detemined by the dynamic pricing, which means you can directly negotiate up to your satisfaction.","title":"Price"},{"location":"course/#about-daisuke-kuwabara","text":"- Data Specialist at EPAM Systems - Master in Data Science - Google Cloud Professional Data Engineer - AWS Solution Architect Associates","title":"About Daisuke Kuwabara"},{"location":"daisuke/","text":"After finishing my undergraduate degree in Area Studies where I specialize in Big Data in Africa, I continue to study MSc of Data Science in Data ScienceTech Institute and graduate in the first class with an honor. During my undergraduate I also worked for Amazon's startup project(Amazon Explorer) in Tokyo as beta team to test out the product and contribute to sorting out potential edge cases. During my master, I have worked for Japanese startup as Data Scientist to convey PoC project where I clean/prepare/model/validate/test data and as Compute Vision Engineer to deploy deepsort(Object Tracking) into the edge device(NVIDIA Jetson) by converting the configuration file used for the core algorithm with the open-source memory-less framework. Currently working as cloud support engineer in Data Shard at EPAM Systems where handling the BigData/DataScience/MachineLearning/AI inquiries(those minors to the criticals) from the enterprises including those from Fortune 500 and FAANG by analyzing logs, metrics, source code and reproducing an infrastructure with the ticket system with the corresponding SLA/SLO. Born in Japan, traveling around the world since 16 years old, especially several years in France where I stay in the south France city called Montpellier . My imagination is from the inspiration in every moment I live, feel, experience materliazed into the creative entity. Experiences Full Stack Data Scientist at EPAM Systems Data Scientist & Computer Vision Engineer at Startup MSc in Data Science & Artificial Intelligence at Data ScienceTech Institute Contact Email: daisuke0582@gmail.com Phone: +81 90 9293 4580","title":"About Daisuke Kuwabara"},{"location":"daisuke/#experiences","text":"Full Stack Data Scientist at EPAM Systems Data Scientist & Computer Vision Engineer at Startup MSc in Data Science & Artificial Intelligence at Data ScienceTech Institute","title":"Experiences"},{"location":"daisuke/#contact","text":"Email: daisuke0582@gmail.com Phone: +81 90 9293 4580","title":"Contact"},{"location":"blog/airflow/","text":"Airflow Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. You could orchestrate the task as Directed Acyclic Graph data_orchestration.py from airflow import models from airflow.hooks.base import BaseHook from airflow.providers.google.cloud.operators.bigquery import BigQueryCheckOperator from airflow.providers.google.cloud.operators.dataflow import DataflowTemplatedJobStartOperator from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator from airflow.utils.dates import days_ago from airflow.utils.state import State # Sample data BUCKET_NAME = \"cloud-samples-data/composer/data-orchestration-blog-example\" DATA_FILE_NAME = \"bike_station_data.csv\" # Assumes existence of the following Airflow Variables PROJECT_ID = models . Variable . get ( \"gcp_project\" ) DATASET = models . Variable . get ( \"bigquery_dataset\" ) TABLE = models . Variable . get ( \"bigquery_table\" ) # Slack error notification example taken from Kaxil Naik's blog on Slack Integration: # https://medium.com/datareply/integrating-slack-alerts-in-airflow-c9dcd155105 def on_failure_callback ( context ): ti = context . get ( \"task_instance\" ) slack_msg = f \"\"\" :red_circle: Task Failed. *Task*: { ti . task_id } *Dag*: { ti . dag_id } *Execution Time*: { context . get ( 'execution_date' ) } *Log Url*: { ti . log_url } \"\"\" slack_webhook_token = BaseHook . get_connection ( \"slack_connection\" ) . password slack_error = SlackWebhookOperator ( task_id = \"post_slack_error\" , http_conn_id = \"slack_connection\" , channel = \"#airflow-alerts\" , webhook_token = slack_webhook_token , message = slack_msg , ) slack_error . execute ( context ) with models . DAG ( \"dataflow_to_bq_workflow\" , schedule_interval = None , start_date = days_ago ( 1 ), default_args = { \"on_failure_callback\" : on_failure_callback }, ) as dag : validate_file_exists = GCSObjectExistenceSensor ( task_id = \"validate_file_exists\" , bucket = BUCKET_NAME , object = DATA_FILE_NAME ) # See Launching Dataflow pipelines with Cloud Composer tutorial for further guidance # https://cloud.google.com/composer/docs/how-to/using/using-dataflow-template-operator start_dataflow_job = DataflowTemplatedJobStartOperator ( task_id = \"start-dataflow-template-job\" , job_name = \"csv_to_bq_transform\" , template = \"gs://dataflow-templates/latest/GCS_Text_to_BigQuery\" , parameters = { \"javascriptTextTransformFunctionName\" : \"transform\" , \"javascriptTextTransformGcsPath\" : f \"gs:// { BUCKET_NAME } /udf_transform.js\" , \"JSONPath\" : f \"gs:// { BUCKET_NAME } /bq_schema.json\" , \"inputFilePattern\" : f \"gs:// { BUCKET_NAME } / { DATA_FILE_NAME } \" , \"bigQueryLoadingTemporaryDirectory\" : f \"gs:// { BUCKET_NAME } /tmp/\" , \"outputTable\" : f \" { PROJECT_ID } : { DATASET } . { TABLE } \" , }, ) execute_bigquery_sql = BigQueryCheckOperator ( task_id = \"execute_bigquery_sql\" , sql = f \"SELECT COUNT(*) FROM ` { PROJECT_ID } . { DATASET } . { TABLE } `\" , use_legacy_sql = False , ) validate_file_exists >> start_dataflow_job >> execute_bigquery_sql if __name__ == \"__main__\" : dag . clear ( dag_run_state = State . NONE ) dag . run () Approach Astronomer Astronomer is the commercial developer of Apache Airflow, a community-driven open-source tool that's leading the market in data orchestration. We're a globally-distributed and rapidly growing venture-backed team of learners, innovators and collaborators. Cloud Composer Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow in Google Cloud Platform Amazon Managed Workflows for Apache Airflow(MWAA) Amazon Managed Workflows for Apache Airflow (MWAA) is a managed orchestration service for Apache Airflow1 that makes it easier to set up and operate end-to-end data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as \u201cworkflows.\u201d in Amazon Web Service.","title":"Airflow"},{"location":"blog/airflow/#airflow","text":"Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. You could orchestrate the task as Directed Acyclic Graph data_orchestration.py from airflow import models from airflow.hooks.base import BaseHook from airflow.providers.google.cloud.operators.bigquery import BigQueryCheckOperator from airflow.providers.google.cloud.operators.dataflow import DataflowTemplatedJobStartOperator from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator from airflow.utils.dates import days_ago from airflow.utils.state import State # Sample data BUCKET_NAME = \"cloud-samples-data/composer/data-orchestration-blog-example\" DATA_FILE_NAME = \"bike_station_data.csv\" # Assumes existence of the following Airflow Variables PROJECT_ID = models . Variable . get ( \"gcp_project\" ) DATASET = models . Variable . get ( \"bigquery_dataset\" ) TABLE = models . Variable . get ( \"bigquery_table\" ) # Slack error notification example taken from Kaxil Naik's blog on Slack Integration: # https://medium.com/datareply/integrating-slack-alerts-in-airflow-c9dcd155105 def on_failure_callback ( context ): ti = context . get ( \"task_instance\" ) slack_msg = f \"\"\" :red_circle: Task Failed. *Task*: { ti . task_id } *Dag*: { ti . dag_id } *Execution Time*: { context . get ( 'execution_date' ) } *Log Url*: { ti . log_url } \"\"\" slack_webhook_token = BaseHook . get_connection ( \"slack_connection\" ) . password slack_error = SlackWebhookOperator ( task_id = \"post_slack_error\" , http_conn_id = \"slack_connection\" , channel = \"#airflow-alerts\" , webhook_token = slack_webhook_token , message = slack_msg , ) slack_error . execute ( context ) with models . DAG ( \"dataflow_to_bq_workflow\" , schedule_interval = None , start_date = days_ago ( 1 ), default_args = { \"on_failure_callback\" : on_failure_callback }, ) as dag : validate_file_exists = GCSObjectExistenceSensor ( task_id = \"validate_file_exists\" , bucket = BUCKET_NAME , object = DATA_FILE_NAME ) # See Launching Dataflow pipelines with Cloud Composer tutorial for further guidance # https://cloud.google.com/composer/docs/how-to/using/using-dataflow-template-operator start_dataflow_job = DataflowTemplatedJobStartOperator ( task_id = \"start-dataflow-template-job\" , job_name = \"csv_to_bq_transform\" , template = \"gs://dataflow-templates/latest/GCS_Text_to_BigQuery\" , parameters = { \"javascriptTextTransformFunctionName\" : \"transform\" , \"javascriptTextTransformGcsPath\" : f \"gs:// { BUCKET_NAME } /udf_transform.js\" , \"JSONPath\" : f \"gs:// { BUCKET_NAME } /bq_schema.json\" , \"inputFilePattern\" : f \"gs:// { BUCKET_NAME } / { DATA_FILE_NAME } \" , \"bigQueryLoadingTemporaryDirectory\" : f \"gs:// { BUCKET_NAME } /tmp/\" , \"outputTable\" : f \" { PROJECT_ID } : { DATASET } . { TABLE } \" , }, ) execute_bigquery_sql = BigQueryCheckOperator ( task_id = \"execute_bigquery_sql\" , sql = f \"SELECT COUNT(*) FROM ` { PROJECT_ID } . { DATASET } . { TABLE } `\" , use_legacy_sql = False , ) validate_file_exists >> start_dataflow_job >> execute_bigquery_sql if __name__ == \"__main__\" : dag . clear ( dag_run_state = State . NONE ) dag . run ()","title":"Airflow"},{"location":"blog/airflow/#approach","text":"Astronomer Astronomer is the commercial developer of Apache Airflow, a community-driven open-source tool that's leading the market in data orchestration. We're a globally-distributed and rapidly growing venture-backed team of learners, innovators and collaborators. Cloud Composer Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow in Google Cloud Platform Amazon Managed Workflows for Apache Airflow(MWAA) Amazon Managed Workflows for Apache Airflow (MWAA) is a managed orchestration service for Apache Airflow1 that makes it easier to set up and operate end-to-end data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as \u201cworkflows.\u201d in Amazon Web Service.","title":"Approach"},{"location":"blog/babushka/","text":"Why Babushka? Old woman holding vegetables is wearing Babushka Babushka refers to a woman's scarf, as well as an elderly Russian woman. The symbolic trend of Babushka is widely known as Babushka Boi by A$AP Rocky. The cultural tribute is the center of Babushka ML . Machine Learning is the heritage of thousands years of study in the various field, blooming by virtue of modern hardware development. Babushka ML is established on the heritage of our progressivity and ingenuity.","title":"Why Babushka?"},{"location":"blog/babushka/#why-babushka","text":"Old woman holding vegetables is wearing Babushka Babushka refers to a woman's scarf, as well as an elderly Russian woman. The symbolic trend of Babushka is widely known as Babushka Boi by A$AP Rocky. The cultural tribute is the center of Babushka ML . Machine Learning is the heritage of thousands years of study in the various field, blooming by virtue of modern hardware development. Babushka ML is established on the heritage of our progressivity and ingenuity.","title":"Why Babushka?"},{"location":"blog/beam/","text":"data_pipeline.py import argparse import logging import json import time import traceback from typing import Optional , List import apache_beam as beam from apache_beam import Create , Map , ParDo , Flatten , Keys from apache_beam import Values , GroupByKey , CoGroupByKey , CombineGlobally , CombinePerKey from apache_beam import pvalue , window , WindowInto from apache_beam.transforms.util import WithKeys from apache_beam.transforms.combiners import Top , Mean from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options import pipeline_options from apache_beam.io.gcp.bigquery import BigQueryDisposition , WriteToBigQuery from apache_beam.io.textio import , ReadFromText , WriteToText def run ( beam_args : Optional [ List [ str ]] = None ): pipeline_options = PipelineOptions ( beam_args ) with beam . Pipeline ( options = pipeline_options , runner = \"DataflowRunner\" ) as p : # File paths relative to the bucket data = ( p | \"Create file path tuple\" >> ReadFromText ( \"gs://bucket/test_file\" ) | \"JSON MAP\" >> beam . Map ( json . loads )) bq = data | \"WriteToBigQuery\" >> WriteToBigQuery ( table = \"project:beam_basics.test1\" , schema = \"SCHEMA_AUTODETECT\" , create_disposition = BigQueryDisposition . CREATE_IF_NEEDED , write_disposition = BigQueryDisposition . WRITE_APPEND ) gcs = data | \"WriteToGCS\" >> WriteToText ( file_path_prefix = \"gs://bucket/output/test_file_out\" , ) if __name__ == '__main__' : logging . getLogger () . setLevel ( logging . INFO ) parser = argparse . ArgumentParser () #parser.add_argument('--input', default = '') #parser.add_argument('--output', required =True) known_args , pipeline_args = parser . parse_known_args () run ( pipeline_args )","title":"Beam"},{"location":"blog/data_engineer/","text":"Data Engieer Architecture Reading time: 5 minutes There has been a surge of interest in the metrics layer, a system providing a standard set of definitions on top of the data warehouse. This has been hotly debated, including what capabilities it should have, which vendor(s) should own it, and what spec it should follow. So far, we\u2019ve seen several credible pure-play products (like Transform and Supergrain), plus expansion into this category by dbt. Reverse ETL vendors have grown meaningfully, particularly Hightouch and Census. The purpose of these products is to update operational systems, like CRM or ERP, with outputs and insights derived from the data warehouse. Data teams are showing stronger interest in new applications to augment their standard dashboards, especially data workspaces (like Hex). Broadly speaking, new apps are likely the result of increasing standardization in cloud data warehouses \u2014 once data is cleanly structured and easy to access, data teams naturally want to do more with it. Data discovery and observability companies have proliferated and raised substantial amounts of capital (especially Monte Carlo and Bigeye). While the benefits of these products are clear \u2014 i.e. more reliable data pipelines and better collaboration \u2014 adoption is still relatively early, as customers discover relevant use cases and budgets. (Technical note: although there are several credible new vendors in data discovery \u2014 e.g. Select Star, Metaphor, Stemma, Secoda, Castor \u2014 we have excluded seed-stage companies from the diagram in general.) There is growing recognition and clarity for the lakehouse architecture. We\u2019ve seen this approach supported by a wide range of vendors (including AWS, Databricks, Google Cloud, Starburst, and Dremio) and data warehouse pioneers. The fundamental value of the lakehouse is to pair a robust storage layer with an array of powerful data processing engines like Spark, Presto, Druid/Clickhouse, Python libraries, etc. The storage layer itself is getting an upgrade. While technologies like Delta, Iceberg, and Hudi are not new, they are seeing accelerated adoption and are being built into commercial products. Some of these technologies (particularly Iceberg) also interoperate with cloud data warehouses like Snowflake. If heterogeneity is here to stay, this is likely to become a key part of the multimodal data stack. There may be an uptick in adoption taking place for stream processing (i.e., real-time analytical data processing). While first-generation technologies like Flink still haven\u2019t gone mainstream, new entrants with simpler programming models (like Materialize and Upsolver) are gaining early adoption, and, anecdotally, usage of stream processing products from incumbents Databricks and Confluent has also started to accelerate. The ML industry is consolidating around a data-centric approach, emphasizing sophisticated data management over incremental modeling improvements. This has several implications: Rapid growth for data labeling (e.g. Scale and Labelbox) and growing interest in closed-loop data engines, largely modeled on Tesla\u2019s Autopilot data pipelines. Increased adoption for feature stores (e.g. Tecton), for both batch and real-time use cases, as a means to develop production-grade ML data in a collaborative way. Revived interest in low-code ML solutions (like Continual and MindsDB) that at least partially automate the ML modeling process. These newer solutions focus on bringing new users (i.e. analysts and software developers) into the ML market. Use of pre-trained models is becoming the default, especially in NLP, and providing tailwinds to companies like OpenAI and Hugging Face. There are still meaningful problems to solve here around fine-tuning, cost, and scaling. Operations tools for ML (sometimes called MLops) are becoming more mature, built around ML monitoring as the most in-demand use case and immediate budget. Meanwhile, a raft of new operational tools \u2014 including, notably, validation and auditing \u2014 are appearing, with the ultimate market still to be determined. There is increased focus on how developers can seamlessly integrate ML models into applications, including through pre-built APIs (e.g. OpenAI), vector databases (e.g. Pinecone), and more opinionated frameworks. Source: https://future.a16z.com/emerging-architectures-modern-data-infrastructure/","title":"Data Engineer"},{"location":"blog/data_engineer/#data-engieer-architecture","text":"Reading time: 5 minutes There has been a surge of interest in the metrics layer, a system providing a standard set of definitions on top of the data warehouse. This has been hotly debated, including what capabilities it should have, which vendor(s) should own it, and what spec it should follow. So far, we\u2019ve seen several credible pure-play products (like Transform and Supergrain), plus expansion into this category by dbt. Reverse ETL vendors have grown meaningfully, particularly Hightouch and Census. The purpose of these products is to update operational systems, like CRM or ERP, with outputs and insights derived from the data warehouse. Data teams are showing stronger interest in new applications to augment their standard dashboards, especially data workspaces (like Hex). Broadly speaking, new apps are likely the result of increasing standardization in cloud data warehouses \u2014 once data is cleanly structured and easy to access, data teams naturally want to do more with it. Data discovery and observability companies have proliferated and raised substantial amounts of capital (especially Monte Carlo and Bigeye). While the benefits of these products are clear \u2014 i.e. more reliable data pipelines and better collaboration \u2014 adoption is still relatively early, as customers discover relevant use cases and budgets. (Technical note: although there are several credible new vendors in data discovery \u2014 e.g. Select Star, Metaphor, Stemma, Secoda, Castor \u2014 we have excluded seed-stage companies from the diagram in general.) There is growing recognition and clarity for the lakehouse architecture. We\u2019ve seen this approach supported by a wide range of vendors (including AWS, Databricks, Google Cloud, Starburst, and Dremio) and data warehouse pioneers. The fundamental value of the lakehouse is to pair a robust storage layer with an array of powerful data processing engines like Spark, Presto, Druid/Clickhouse, Python libraries, etc. The storage layer itself is getting an upgrade. While technologies like Delta, Iceberg, and Hudi are not new, they are seeing accelerated adoption and are being built into commercial products. Some of these technologies (particularly Iceberg) also interoperate with cloud data warehouses like Snowflake. If heterogeneity is here to stay, this is likely to become a key part of the multimodal data stack. There may be an uptick in adoption taking place for stream processing (i.e., real-time analytical data processing). While first-generation technologies like Flink still haven\u2019t gone mainstream, new entrants with simpler programming models (like Materialize and Upsolver) are gaining early adoption, and, anecdotally, usage of stream processing products from incumbents Databricks and Confluent has also started to accelerate. The ML industry is consolidating around a data-centric approach, emphasizing sophisticated data management over incremental modeling improvements. This has several implications: Rapid growth for data labeling (e.g. Scale and Labelbox) and growing interest in closed-loop data engines, largely modeled on Tesla\u2019s Autopilot data pipelines. Increased adoption for feature stores (e.g. Tecton), for both batch and real-time use cases, as a means to develop production-grade ML data in a collaborative way. Revived interest in low-code ML solutions (like Continual and MindsDB) that at least partially automate the ML modeling process. These newer solutions focus on bringing new users (i.e. analysts and software developers) into the ML market. Use of pre-trained models is becoming the default, especially in NLP, and providing tailwinds to companies like OpenAI and Hugging Face. There are still meaningful problems to solve here around fine-tuning, cost, and scaling. Operations tools for ML (sometimes called MLops) are becoming more mature, built around ML monitoring as the most in-demand use case and immediate budget. Meanwhile, a raft of new operational tools \u2014 including, notably, validation and auditing \u2014 are appearing, with the ultimate market still to be determined. There is increased focus on how developers can seamlessly integrate ML models into applications, including through pre-built APIs (e.g. OpenAI), vector databases (e.g. Pinecone), and more opinionated frameworks. Source: https://future.a16z.com/emerging-architectures-modern-data-infrastructure/","title":"Data Engieer Architecture"},{"location":"blog/tft_beam/","text":"Tensorflow tf.Transform is a library for TensorFlow that allows you to define both instance-level and full-pass data transformations through data preprocessing pipelines. These pipelines are efficiently executed with Apache Beam and they create as byproducts a TensorFlow graph to apply the same transformations during prediction as when the model is served. https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt2 https://www.tensorflow.org/tfx/transform/get_started !sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst !pip install tensorflow == 2 .3.0 tensorflow-transform == 0 .24.0 apache-beam [ gcp ]== 2 .24.0 !pip install --user google-cloud-bigquery == 1 .25.0 !pip download tensorflow-transform == 0 .24.0 --no-deps pip freeze | grep -e 'flow\\|beam' import tensorflow as tf import tensorflow_transform as tft import shutil print ( tf . __version__ ) # change these to try this notebook out BUCKET = 'bucket-name' PROJECT = 'project-id' REGION = 'us-central1' import os os . environ [ 'BUCKET' ] = BUCKET os . environ [ 'PROJECT' ] = PROJECT os . environ [ 'REGION' ] = REGION gcloud config set project $PROJECT gcloud config set compute/region $REGION if ! gsutil ls | grep -q gs:// ${ BUCKET } / ; then gsutil mb -l ${ REGION } gs:// ${ BUCKET } fi from google.cloud import bigquery def create_query ( phase , EVERY_N ): \"\"\"Creates a query with the proper splits. Args: phase: int, 1=train, 2=valid. EVERY_N: int, take an example EVERY_N rows. Returns: Query string with the proper splits. \"\"\" base_query = \"\"\" WITH daynames AS (SELECT ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek) SELECT (tolls_amount + fare_amount) AS fare_amount, daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek, EXTRACT(HOUR FROM pickup_datetime) AS hourofday, pickup_longitude AS pickuplon, pickup_latitude AS pickuplat, dropoff_longitude AS dropofflon, dropoff_latitude AS dropofflat, passenger_count AS passengers, 'notneeded' AS key FROM `nyc-tlc.yellow.trips`, daynames WHERE trip_distance > 0 AND fare_amount > 0 \"\"\" if EVERY_N is None : if phase < 2 : # training query = \"\"\" {0} AND ABS(MOD(FARM_FINGERPRINT(CAST (pickup_datetime AS STRING), 4)) < 2\"\"\" . format ( base_query ) else : query = \"\"\" {0} AND ABS(MOD(FARM_FINGERPRINT(CAST( pickup_datetime AS STRING), 4)) = {1} \"\"\" . format ( base_query , phase ) else : query = \"\"\" {0} AND ABS(MOD(FARM_FINGERPRINT(CAST( pickup_datetime AS STRING)), {1} )) = {2} \"\"\" . format ( base_query , EVERY_N , phase ) return query query = create_query ( 2 , 100000 ) # Import a module named `datetime` to work with dates as date objects. import datetime # Import data processing libraries and modules import tensorflow as tf import apache_beam as beam import tensorflow_transform as tft import tensorflow_metadata as tfmd from tensorflow_transform.beam import impl as beam_impl def is_valid ( inputs ): \"\"\"Check to make sure the inputs are valid. Args: inputs: dict, dictionary of TableRow data from BigQuery. Returns: True if the inputs are valid and False if they are not. \"\"\" try : pickup_longitude = inputs [ 'pickuplon' ] dropoff_longitude = inputs [ 'dropofflon' ] pickup_latitude = inputs [ 'pickuplat' ] dropoff_latitude = inputs [ 'dropofflat' ] hourofday = inputs [ 'hourofday' ] dayofweek = inputs [ 'dayofweek' ] passenger_count = inputs [ 'passengers' ] fare_amount = inputs [ 'fare_amount' ] return fare_amount >= 2.5 and pickup_longitude > - 78 \\ and pickup_longitude < - 70 and dropoff_longitude > - 78 \\ and dropoff_longitude < - 70 and pickup_latitude > 37 \\ and pickup_latitude < 45 and dropoff_latitude > 37 \\ and dropoff_latitude < 45 and passenger_count > 0 except : return False def preprocess_tft ( inputs ): \"\"\"Preprocess the features and add engineered features with tf transform. Args: dict, dictionary of TableRow data from BigQuery. Returns: Dictionary of preprocessed data after scaling and feature engineering. \"\"\" import datetime print ( inputs ) result = {} result [ 'fare_amount' ] = tf . identity ( inputs [ 'fare_amount' ]) # build a vocabulary result [ 'dayofweek' ] = tft . string_to_int ( inputs [ 'dayofweek' ]) result [ 'hourofday' ] = tf . identity ( inputs [ 'hourofday' ]) # pass through # scaling numeric values result [ 'pickuplon' ] = ( tft . scale_to_0_1 ( inputs [ 'pickuplon' ])) result [ 'pickuplat' ] = ( tft . scale_to_0_1 ( inputs [ 'pickuplat' ])) result [ 'dropofflon' ] = ( tft . scale_to_0_1 ( inputs [ 'dropofflon' ])) result [ 'dropofflat' ] = ( tft . scale_to_0_1 ( inputs [ 'dropofflat' ])) result [ 'passengers' ] = tf . cast ( inputs [ 'passengers' ], tf . float32 ) # a cast # arbitrary TF func result [ 'key' ] = tf . as_string ( tf . ones_like ( inputs [ 'passengers' ])) # engineered features latdiff = inputs [ 'pickuplat' ] - inputs [ 'dropofflat' ] londiff = inputs [ 'pickuplon' ] - inputs [ 'dropofflon' ] # Scale our engineered features latdiff and londiff between 0 and 1 result [ 'latdiff' ] = tft . scale_to_0_1 ( latdiff ) result [ 'londiff' ] = tft . scale_to_0_1 ( londiff ) dist = tf . sqrt ( latdiff * latdiff + londiff * londiff ) result [ 'euclidean' ] = tft . scale_to_0_1 ( dist ) return result def preprocess ( in_test_mode ): \"\"\"Sets up preprocess pipeline. Args: in_test_mode: bool, False to launch DataFlow job, True to run locally. \"\"\" import os import os.path import tempfile from apache_beam.io import tfrecordio from tensorflow_transform.coders import example_proto_coder from tensorflow_transform.tf_metadata import dataset_metadata from tensorflow_transform.tf_metadata import dataset_schema from tensorflow_transform.beam import tft_beam_io from tensorflow_transform.beam.tft_beam_io import transform_fn_io job_name = 'preprocess-taxi-features' + '-' job_name += datetime . datetime . now () . strftime ( '%y%m %d -%H%M%S' ) if in_test_mode : import shutil print ( 'Launching local job ... hang on' ) OUTPUT_DIR = './preproc_tft' shutil . rmtree ( OUTPUT_DIR , ignore_errors = True ) EVERY_N = 100000 else : print ( 'Launching Dataflow job {} ... hang on' . format ( job_name )) OUTPUT_DIR = 'gs:// {0} /taxifare/preproc_tft/' . format ( BUCKET ) import subprocess subprocess . call ( 'gsutil rm -r {} ' . format ( OUTPUT_DIR ) . split ()) EVERY_N = 10000 options = { 'staging_location' : os . path . join ( OUTPUT_DIR , 'tmp' , 'staging' ), 'temp_location' : os . path . join ( OUTPUT_DIR , 'tmp' ), 'job_name' : job_name , 'project' : PROJECT , 'num_workers' : 1 , 'max_num_workers' : 1 , 'teardown_policy' : 'TEARDOWN_ALWAYS' , 'no_save_main_session' : True , 'direct_num_workers' : 1 , 'extra_packages' : [ 'tensorflow_transform-0.24.0-py3-none-any.whl' ] } opts = beam . pipeline . PipelineOptions ( flags = [], ** options ) if in_test_mode : RUNNER = 'DirectRunner' else : RUNNER = 'DataflowRunner' # Set up raw data metadata raw_data_schema = { colname : dataset_schema . ColumnSchema ( tf . string , [], dataset_schema . FixedColumnRepresentation ()) for colname in 'dayofweek,key' . split ( ',' ) } raw_data_schema . update ({ colname : dataset_schema . ColumnSchema ( tf . float32 , [], dataset_schema . FixedColumnRepresentation ()) for colname in 'fare_amount,pickuplon,pickuplat,dropofflon,dropofflat' . split ( ',' ) }) raw_data_schema . update ({ colname : dataset_schema . ColumnSchema ( tf . int64 , [], dataset_schema . FixedColumnRepresentation ()) for colname in 'hourofday,passengers' . split ( ',' ) }) raw_data_metadata = dataset_metadata . DatasetMetadata ( dataset_schema . Schema ( raw_data_schema )) # Run Beam with beam . Pipeline ( RUNNER , options = opts ) as p : with beam_impl . Context ( temp_dir = os . path . join ( OUTPUT_DIR , 'tmp' )): # Save the raw data metadata ( raw_data_metadata | 'WriteInputMetadata' >> tft_beam_io . WriteMetadata ( os . path . join ( OUTPUT_DIR , 'metadata/rawdata_metadata' ), pipeline = p )) # Read training data from bigquery and filter rows raw_data = ( p | 'train_read' >> beam . io . Read ( beam . io . BigQuerySource ( query = create_query ( 1 , EVERY_N ), use_standard_sql = True )) | 'train_filter' >> beam . Filter ( is_valid )) raw_dataset = ( raw_data , raw_data_metadata ) # Analyze and transform training data transformed_dataset , transform_fn = ( raw_dataset | beam_impl . AnalyzeAndTransformDataset ( preprocess_tft )) transformed_data , transformed_metadata = transformed_dataset # Save transformed train data to disk in efficient tfrecord format transformed_data | 'WriteTrainData' >> tfrecordio . WriteToTFRecord ( os . path . join ( OUTPUT_DIR , 'train' ), file_name_suffix = '.gz' , coder = example_proto_coder . ExampleProtoCoder ( transformed_metadata . schema )) # Read eval data from bigquery and filter rows raw_test_data = ( p | 'eval_read' >> beam . io . Read ( beam . io . BigQuerySource ( query = create_query ( 2 , EVERY_N ), use_standard_sql = True )) | 'eval_filter' >> beam . Filter ( is_valid )) raw_test_dataset = ( raw_test_data , raw_data_metadata ) # Transform eval data transformed_test_dataset = ( ( raw_test_dataset , transform_fn ) | beam_impl . TransformDataset () ) transformed_test_data , _ = transformed_test_dataset # Save transformed train data to disk in efficient tfrecord format ( transformed_test_data | 'WriteTestData' >> tfrecordio . WriteToTFRecord ( os . path . join ( OUTPUT_DIR , 'eval' ), file_name_suffix = '.gz' , coder = example_proto_coder . ExampleProtoCoder ( transformed_metadata . schema ))) # Save transformation function to disk for use at serving time ( transform_fn | 'WriteTransformFn' >> transform_fn_io . WriteTransformFn ( os . path . join ( OUTPUT_DIR , 'metadata' ))) # Change to True to run locally preprocess ( in_test_mode = False ) Check out Notebook for the complete flow.","title":"Using Tensorflow Transform in Apache Beam"},{"location":"blog/tft_beam/#tensorflow","text":"tf.Transform is a library for TensorFlow that allows you to define both instance-level and full-pass data transformations through data preprocessing pipelines. These pipelines are efficiently executed with Apache Beam and they create as byproducts a TensorFlow graph to apply the same transformations during prediction as when the model is served. https://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt2 https://www.tensorflow.org/tfx/transform/get_started !sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst !pip install tensorflow == 2 .3.0 tensorflow-transform == 0 .24.0 apache-beam [ gcp ]== 2 .24.0 !pip install --user google-cloud-bigquery == 1 .25.0 !pip download tensorflow-transform == 0 .24.0 --no-deps pip freeze | grep -e 'flow\\|beam' import tensorflow as tf import tensorflow_transform as tft import shutil print ( tf . __version__ ) # change these to try this notebook out BUCKET = 'bucket-name' PROJECT = 'project-id' REGION = 'us-central1' import os os . environ [ 'BUCKET' ] = BUCKET os . environ [ 'PROJECT' ] = PROJECT os . environ [ 'REGION' ] = REGION gcloud config set project $PROJECT gcloud config set compute/region $REGION if ! gsutil ls | grep -q gs:// ${ BUCKET } / ; then gsutil mb -l ${ REGION } gs:// ${ BUCKET } fi from google.cloud import bigquery def create_query ( phase , EVERY_N ): \"\"\"Creates a query with the proper splits. Args: phase: int, 1=train, 2=valid. EVERY_N: int, take an example EVERY_N rows. Returns: Query string with the proper splits. \"\"\" base_query = \"\"\" WITH daynames AS (SELECT ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek) SELECT (tolls_amount + fare_amount) AS fare_amount, daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek, EXTRACT(HOUR FROM pickup_datetime) AS hourofday, pickup_longitude AS pickuplon, pickup_latitude AS pickuplat, dropoff_longitude AS dropofflon, dropoff_latitude AS dropofflat, passenger_count AS passengers, 'notneeded' AS key FROM `nyc-tlc.yellow.trips`, daynames WHERE trip_distance > 0 AND fare_amount > 0 \"\"\" if EVERY_N is None : if phase < 2 : # training query = \"\"\" {0} AND ABS(MOD(FARM_FINGERPRINT(CAST (pickup_datetime AS STRING), 4)) < 2\"\"\" . format ( base_query ) else : query = \"\"\" {0} AND ABS(MOD(FARM_FINGERPRINT(CAST( pickup_datetime AS STRING), 4)) = {1} \"\"\" . format ( base_query , phase ) else : query = \"\"\" {0} AND ABS(MOD(FARM_FINGERPRINT(CAST( pickup_datetime AS STRING)), {1} )) = {2} \"\"\" . format ( base_query , EVERY_N , phase ) return query query = create_query ( 2 , 100000 ) # Import a module named `datetime` to work with dates as date objects. import datetime # Import data processing libraries and modules import tensorflow as tf import apache_beam as beam import tensorflow_transform as tft import tensorflow_metadata as tfmd from tensorflow_transform.beam import impl as beam_impl def is_valid ( inputs ): \"\"\"Check to make sure the inputs are valid. Args: inputs: dict, dictionary of TableRow data from BigQuery. Returns: True if the inputs are valid and False if they are not. \"\"\" try : pickup_longitude = inputs [ 'pickuplon' ] dropoff_longitude = inputs [ 'dropofflon' ] pickup_latitude = inputs [ 'pickuplat' ] dropoff_latitude = inputs [ 'dropofflat' ] hourofday = inputs [ 'hourofday' ] dayofweek = inputs [ 'dayofweek' ] passenger_count = inputs [ 'passengers' ] fare_amount = inputs [ 'fare_amount' ] return fare_amount >= 2.5 and pickup_longitude > - 78 \\ and pickup_longitude < - 70 and dropoff_longitude > - 78 \\ and dropoff_longitude < - 70 and pickup_latitude > 37 \\ and pickup_latitude < 45 and dropoff_latitude > 37 \\ and dropoff_latitude < 45 and passenger_count > 0 except : return False def preprocess_tft ( inputs ): \"\"\"Preprocess the features and add engineered features with tf transform. Args: dict, dictionary of TableRow data from BigQuery. Returns: Dictionary of preprocessed data after scaling and feature engineering. \"\"\" import datetime print ( inputs ) result = {} result [ 'fare_amount' ] = tf . identity ( inputs [ 'fare_amount' ]) # build a vocabulary result [ 'dayofweek' ] = tft . string_to_int ( inputs [ 'dayofweek' ]) result [ 'hourofday' ] = tf . identity ( inputs [ 'hourofday' ]) # pass through # scaling numeric values result [ 'pickuplon' ] = ( tft . scale_to_0_1 ( inputs [ 'pickuplon' ])) result [ 'pickuplat' ] = ( tft . scale_to_0_1 ( inputs [ 'pickuplat' ])) result [ 'dropofflon' ] = ( tft . scale_to_0_1 ( inputs [ 'dropofflon' ])) result [ 'dropofflat' ] = ( tft . scale_to_0_1 ( inputs [ 'dropofflat' ])) result [ 'passengers' ] = tf . cast ( inputs [ 'passengers' ], tf . float32 ) # a cast # arbitrary TF func result [ 'key' ] = tf . as_string ( tf . ones_like ( inputs [ 'passengers' ])) # engineered features latdiff = inputs [ 'pickuplat' ] - inputs [ 'dropofflat' ] londiff = inputs [ 'pickuplon' ] - inputs [ 'dropofflon' ] # Scale our engineered features latdiff and londiff between 0 and 1 result [ 'latdiff' ] = tft . scale_to_0_1 ( latdiff ) result [ 'londiff' ] = tft . scale_to_0_1 ( londiff ) dist = tf . sqrt ( latdiff * latdiff + londiff * londiff ) result [ 'euclidean' ] = tft . scale_to_0_1 ( dist ) return result def preprocess ( in_test_mode ): \"\"\"Sets up preprocess pipeline. Args: in_test_mode: bool, False to launch DataFlow job, True to run locally. \"\"\" import os import os.path import tempfile from apache_beam.io import tfrecordio from tensorflow_transform.coders import example_proto_coder from tensorflow_transform.tf_metadata import dataset_metadata from tensorflow_transform.tf_metadata import dataset_schema from tensorflow_transform.beam import tft_beam_io from tensorflow_transform.beam.tft_beam_io import transform_fn_io job_name = 'preprocess-taxi-features' + '-' job_name += datetime . datetime . now () . strftime ( '%y%m %d -%H%M%S' ) if in_test_mode : import shutil print ( 'Launching local job ... hang on' ) OUTPUT_DIR = './preproc_tft' shutil . rmtree ( OUTPUT_DIR , ignore_errors = True ) EVERY_N = 100000 else : print ( 'Launching Dataflow job {} ... hang on' . format ( job_name )) OUTPUT_DIR = 'gs:// {0} /taxifare/preproc_tft/' . format ( BUCKET ) import subprocess subprocess . call ( 'gsutil rm -r {} ' . format ( OUTPUT_DIR ) . split ()) EVERY_N = 10000 options = { 'staging_location' : os . path . join ( OUTPUT_DIR , 'tmp' , 'staging' ), 'temp_location' : os . path . join ( OUTPUT_DIR , 'tmp' ), 'job_name' : job_name , 'project' : PROJECT , 'num_workers' : 1 , 'max_num_workers' : 1 , 'teardown_policy' : 'TEARDOWN_ALWAYS' , 'no_save_main_session' : True , 'direct_num_workers' : 1 , 'extra_packages' : [ 'tensorflow_transform-0.24.0-py3-none-any.whl' ] } opts = beam . pipeline . PipelineOptions ( flags = [], ** options ) if in_test_mode : RUNNER = 'DirectRunner' else : RUNNER = 'DataflowRunner' # Set up raw data metadata raw_data_schema = { colname : dataset_schema . ColumnSchema ( tf . string , [], dataset_schema . FixedColumnRepresentation ()) for colname in 'dayofweek,key' . split ( ',' ) } raw_data_schema . update ({ colname : dataset_schema . ColumnSchema ( tf . float32 , [], dataset_schema . FixedColumnRepresentation ()) for colname in 'fare_amount,pickuplon,pickuplat,dropofflon,dropofflat' . split ( ',' ) }) raw_data_schema . update ({ colname : dataset_schema . ColumnSchema ( tf . int64 , [], dataset_schema . FixedColumnRepresentation ()) for colname in 'hourofday,passengers' . split ( ',' ) }) raw_data_metadata = dataset_metadata . DatasetMetadata ( dataset_schema . Schema ( raw_data_schema )) # Run Beam with beam . Pipeline ( RUNNER , options = opts ) as p : with beam_impl . Context ( temp_dir = os . path . join ( OUTPUT_DIR , 'tmp' )): # Save the raw data metadata ( raw_data_metadata | 'WriteInputMetadata' >> tft_beam_io . WriteMetadata ( os . path . join ( OUTPUT_DIR , 'metadata/rawdata_metadata' ), pipeline = p )) # Read training data from bigquery and filter rows raw_data = ( p | 'train_read' >> beam . io . Read ( beam . io . BigQuerySource ( query = create_query ( 1 , EVERY_N ), use_standard_sql = True )) | 'train_filter' >> beam . Filter ( is_valid )) raw_dataset = ( raw_data , raw_data_metadata ) # Analyze and transform training data transformed_dataset , transform_fn = ( raw_dataset | beam_impl . AnalyzeAndTransformDataset ( preprocess_tft )) transformed_data , transformed_metadata = transformed_dataset # Save transformed train data to disk in efficient tfrecord format transformed_data | 'WriteTrainData' >> tfrecordio . WriteToTFRecord ( os . path . join ( OUTPUT_DIR , 'train' ), file_name_suffix = '.gz' , coder = example_proto_coder . ExampleProtoCoder ( transformed_metadata . schema )) # Read eval data from bigquery and filter rows raw_test_data = ( p | 'eval_read' >> beam . io . Read ( beam . io . BigQuerySource ( query = create_query ( 2 , EVERY_N ), use_standard_sql = True )) | 'eval_filter' >> beam . Filter ( is_valid )) raw_test_dataset = ( raw_test_data , raw_data_metadata ) # Transform eval data transformed_test_dataset = ( ( raw_test_dataset , transform_fn ) | beam_impl . TransformDataset () ) transformed_test_data , _ = transformed_test_dataset # Save transformed train data to disk in efficient tfrecord format ( transformed_test_data | 'WriteTestData' >> tfrecordio . WriteToTFRecord ( os . path . join ( OUTPUT_DIR , 'eval' ), file_name_suffix = '.gz' , coder = example_proto_coder . ExampleProtoCoder ( transformed_metadata . schema ))) # Save transformation function to disk for use at serving time ( transform_fn | 'WriteTransformFn' >> transform_fn_io . WriteTransformFn ( os . path . join ( OUTPUT_DIR , 'metadata' ))) # Change to True to run locally preprocess ( in_test_mode = False ) Check out Notebook for the complete flow.","title":"Tensorflow"},{"location":"breakroom/break/","text":"Find the ideal place We have no choice which country to be born, but we can choose which country to spend our life at this very 21 centuries. This page briefly introduces my personal tips for those who are willing to find your best destination country. Towards the free world... Living in Japan Living in France Living in Turkey","title":"Find your ideal place"},{"location":"breakroom/break/#find-the-ideal-place","text":"We have no choice which country to be born, but we can choose which country to spend our life at this very 21 centuries. This page briefly introduces my personal tips for those who are willing to find your best destination country. Towards the free world... Living in Japan Living in France Living in Turkey","title":"Find the ideal place"},{"location":"breakroom/france/","text":"Visa House Jinka Look no further as Jinka offeres the best cartier and coin in your search for the ideal apartment. Food Carrefour Great place to find almost all essentials Monoprix High end Supermarket Clothes Uniqlo is popular among France Transportation I am personally averse towards French Transportation system. They make it enough complicated to make a mistake, and yet charge you for that. I have been once fined in Charles de Gaulle, simply because I use the ticket to airport in the local gate(Note that there are two gates within the same subway... two gate..) Glossary Pain au chocolat is called Chocolatine in south-west of France","title":"Living in France"},{"location":"breakroom/france/#visa","text":"","title":"Visa"},{"location":"breakroom/france/#house","text":"Jinka Look no further as Jinka offeres the best cartier and coin in your search for the ideal apartment.","title":"House"},{"location":"breakroom/france/#food","text":"Carrefour Great place to find almost all essentials Monoprix High end Supermarket","title":"Food"},{"location":"breakroom/france/#clothes","text":"Uniqlo is popular among France","title":"Clothes"},{"location":"breakroom/france/#transportation","text":"I am personally averse towards French Transportation system. They make it enough complicated to make a mistake, and yet charge you for that. I have been once fined in Charles de Gaulle, simply because I use the ticket to airport in the local gate(Note that there are two gates within the same subway... two gate..)","title":"Transportation"},{"location":"breakroom/france/#glossary","text":"Pain au chocolat is called Chocolatine in south-west of France","title":"Glossary"},{"location":"breakroom/japan/","text":"If you are looking for a way/willing to relocate to Japan, feel free to contact me !! I will facilitate your relocation processes completely at ease. Visa Visa Application Procedures House Tokyo is known as one of the most expensive city in the world, which is somewhat accurate to my ear. However, many Japanese people are well familiar with tips to save the cost for finding the suitable house in this city. Oakhouse If you are willing to stay in tokyo in the shared house. Oakhouse is the great place to take a look. They have a large number of real estates across Tokyo, which are oriented to accommodate the foreign citizens. Although Covid-19 had an impact on their business, leading to the down-scaling of their business, I still find the competitive value especially considering the fact that you can stay only with your passport and without \u6577\u91d1(Shikikin) and \u793c\u91d1(Reikin) I myself lives once there and my experience was quite awesome with free morning coffee and launge with fast wifi facility. Home's Though Japanese only website, Home's harnesses Data Science with their real estate business, which allows them to offer relatively comptetitive prices. If you are considering to buy one, this might be a good option as well. Food Yoshinoya Meat rice bowl Otoya Japanese traditional cuisine with modern taste. Starbucks Thanks to many students and nommad workers seeking for their wifi and charge spots, Starbacks find its place in Japan completely perfectly. You could find literally in all cities across Tokyo, and the most of which are well equipped with fast wifi and charge spots. Clothes Transportation Glossary Collecting tech setups are pretty cheap here in Japan \u30d1\u30bd\u30b3\u30f3\u5de5\u623f(pc-koubou) is absolutely great website to find your computer setups. Find the physical location here Trips Beer Onsen(Hot Spring) \u571f\u6e6f\u6e29\u6cc9\uff08Tutchiyu Onsen\uff09 Driving arond 20 minutes, \u571f\u6e6f is located in the valley with bountiful nature of Hukushima prefecture. Note that they even have an accommodation for your remote work 800 yen(7-8 dollars) per day with pretty good wifi(Download: 42.85Mbps Upload: 71.22Mbps ).","title":"Living in Japan"},{"location":"breakroom/japan/#if-you-are-looking-for-a-waywilling-to-relocate-to-japan-feel-free-to-contact-me-i-will-facilitate-your-relocation-processes-completely-at-ease","text":"","title":"If you are looking for a way/willing to relocate to Japan, feel free to contact me!! I will facilitate your relocation processes completely at ease."},{"location":"breakroom/japan/#visa","text":"Visa Application Procedures","title":"Visa"},{"location":"breakroom/japan/#house","text":"Tokyo is known as one of the most expensive city in the world, which is somewhat accurate to my ear. However, many Japanese people are well familiar with tips to save the cost for finding the suitable house in this city. Oakhouse If you are willing to stay in tokyo in the shared house. Oakhouse is the great place to take a look. They have a large number of real estates across Tokyo, which are oriented to accommodate the foreign citizens. Although Covid-19 had an impact on their business, leading to the down-scaling of their business, I still find the competitive value especially considering the fact that you can stay only with your passport and without \u6577\u91d1(Shikikin) and \u793c\u91d1(Reikin) I myself lives once there and my experience was quite awesome with free morning coffee and launge with fast wifi facility. Home's Though Japanese only website, Home's harnesses Data Science with their real estate business, which allows them to offer relatively comptetitive prices. If you are considering to buy one, this might be a good option as well.","title":"House"},{"location":"breakroom/japan/#food","text":"Yoshinoya Meat rice bowl Otoya Japanese traditional cuisine with modern taste. Starbucks Thanks to many students and nommad workers seeking for their wifi and charge spots, Starbacks find its place in Japan completely perfectly. You could find literally in all cities across Tokyo, and the most of which are well equipped with fast wifi and charge spots.","title":"Food"},{"location":"breakroom/japan/#clothes","text":"","title":"Clothes"},{"location":"breakroom/japan/#transportation","text":"","title":"Transportation"},{"location":"breakroom/japan/#glossary","text":"Collecting tech setups are pretty cheap here in Japan \u30d1\u30bd\u30b3\u30f3\u5de5\u623f(pc-koubou) is absolutely great website to find your computer setups. Find the physical location here","title":"Glossary"},{"location":"breakroom/japan/#trips","text":"Beer Onsen(Hot Spring) \u571f\u6e6f\u6e29\u6cc9\uff08Tutchiyu Onsen\uff09 Driving arond 20 minutes, \u571f\u6e6f is located in the valley with bountiful nature of Hukushima prefecture. Note that they even have an accommodation for your remote work 800 yen(7-8 dollars) per day with pretty good wifi(Download: 42.85Mbps Upload: 71.22Mbps ).","title":"Trips"},{"location":"breakroom/turkey/","text":"Visa Residence Permit is all what you need to stay more than the initial stay period. Find apartment(YOU MUST GET notary paper ) Get Turkish Phone Number Get Tax Number Take photos Get health insurance Fill out a form in immigration center and make a reservation House sahibinden.com The most know website where local people are using to find their apartment Food 45 Lira is the average for one meal Clothes Transportation To/From IST Airport - 200 - 300 lira Within Beyoglu - 20 - 35 lira Glossary EspressoLab is the great place for nommad NEVER withdraw your cash at the airport or tourist site. Go to Ziraat where you are charged with the minial extra fee.","title":"Living in Turkey"},{"location":"breakroom/turkey/#visa","text":"Residence Permit is all what you need to stay more than the initial stay period. Find apartment(YOU MUST GET notary paper ) Get Turkish Phone Number Get Tax Number Take photos Get health insurance Fill out a form in immigration center and make a reservation","title":"Visa"},{"location":"breakroom/turkey/#house","text":"sahibinden.com The most know website where local people are using to find their apartment","title":"House"},{"location":"breakroom/turkey/#food","text":"45 Lira is the average for one meal","title":"Food"},{"location":"breakroom/turkey/#clothes","text":"","title":"Clothes"},{"location":"breakroom/turkey/#transportation","text":"To/From IST Airport - 200 - 300 lira Within Beyoglu - 20 - 35 lira","title":"Transportation"},{"location":"breakroom/turkey/#glossary","text":"EspressoLab is the great place for nommad NEVER withdraw your cash at the airport or tourist site. Go to Ziraat where you are charged with the minial extra fee.","title":"Glossary"}]}